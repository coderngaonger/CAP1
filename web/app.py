import streamlit as st
st.set_page_config(page_title="Multi-aspect Comment Classifier", page_icon=":bar_chart:", layout="wide")

import pandas as pd
import numpy as np
import os
import joblib
import re
import emoji
# from num2words import num2words # Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng, c√≥ th·ªÉ b·ªè
# from deep_translator import GoogleTranslator # Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng, c√≥ th·ªÉ b·ªè
import matplotlib.pyplot as plt
# import seaborn as sns # Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng, c√≥ th·ªÉ b·ªè
from wordcloud import WordCloud
# from grouped_wordcloud import GroupedColorFunc # Kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng, c√≥ th·ªÉ b·ªè
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException
from time import sleep
import random
from pathlib import Path
# from joblib import dump, load # joblib ƒë√£ ƒë∆∞·ª£c import ·ªü tr√™n
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from pyvi import ViTokenizer
from gensim.models import Word2Vec
import tensorflow as tf # ƒê·ªÉ t·∫£i m√¥ h√¨nh Keras

os.environ["STREAMLIT_WATCHED_MODULES"] = "false" # N√™n ƒë·∫∑t ·ªü ƒë·∫ßu file

# ==== C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ====
# S·ª≠ d·ª•ng Path cho ƒë∆∞·ªùng d·∫´n ƒë·ªÉ t∆∞∆°ng th√≠ch ƒëa n·ªÅn t·∫£ng t·ªët h∆°n
BASE_APP_DIR = Path(__file__).resolve().parent
MODEL_DIR = BASE_APP_DIR / "model" # V√≠ d·ª•: n·∫øu th∆∞ m·ª•c model ngang h√†ng v·ªõi file script
DATA_DIR = BASE_APP_DIR / "data" # V√≠ d·ª•: n·∫øu th∆∞ m·ª•c data ngang h√†ng v·ªõi file script

# N·∫øu b·∫°n mu·ªën gi·ªØ ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ª´ user:
# model_dir = r"C:\Users\84913\Downloads\TranVanLoc_HoLeKhoiNguyen\model"
# data_base_dir = r"C:\Users\84913\Downloads\TranVanLoc_HoLeKhoiNguyen\data"

# S·ª≠ d·ª•ng MODEL_DIR v√† DATA_DIR ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n s·∫Ω linh ho·∫°t h∆°n
# V√≠ d·ª•:
# model_dir = str(MODEL_DIR) # Chuy·ªÉn Path object th√†nh string n·∫øu c·∫ßn
# abbs_path = DATA_DIR / "abbreviations.xlsx"
# stopwords_path = DATA_DIR / "vietnamesestopwords.txt"

# T·∫°m th·ªùi gi·ªØ l·∫°i ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ª´ ng∆∞·ªùi d√πng ƒë·ªÉ kh·ªõp v·ªõi code g·ªëc
model_dir = r"E:\practice_with_fren\khoinguyen\model"
user_data_dir = r"E:\practice_with_fren\khoinguyen\data" # ƒê·ªÉ t·∫£i abbreviation v√† stopwords

model_files = {
    "price": "PRICE_MODEL.joblib", # Gi·∫£ s·ª≠ PRICE_MODEL l√† Keras model, t√™n file c√≥ th·ªÉ l√† .h5 ho·∫∑c th∆∞ m·ª•c
    "quality": "QUALITY_MODEL.joblib",
    "service": "SERVICE_MODEL.joblib", # Gi·∫£ s·ª≠ SERVICE_MODEL l√† Keras model
    "store": "STORE_MODEL.joblib",
    "packaging": "PACKAGING_MODEL.joblib",
    "others": "OTHERS_MODEL.joblib", # Th√™m d·∫•u ph·∫©y
}

# Create a separate variable for uppercase keys
model_files_keys_uppercase = [k.upper() for k in model_files.keys()]

models = {}
for label, fname in model_files.items():
    path = os.path.join(model_dir, fname)
    try:
        # Gi·∫£ ƒë·ªãnh r·∫±ng PRICE v√† SERVICE l√† c√°c m√¥ h√¨nh Neural Network (Keras)
        # v√† c√°c m√¥ h√¨nh kh√°c l√† scikit-learn (t·∫£i b·∫±ng joblib)
        # B·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh t√™n file ho·∫∑c c√°ch t·∫£i cho ph√π h·ª£p (v√≠ d·ª•: .h5 cho Keras)
        if label in ["price", "service"] and (fname.endswith(".h5") or os.path.isdir(path)): # Ki·ªÉm tra n·∫øu l√† file .h5 ho·∫∑c th∆∞ m·ª•c SavedModel
            models[label] = tf.keras.models.load_model(path)
            print(f"‚Üí Loaded Keras model {label} from {path}")
        else:
            models[label] = joblib.load(path)
            print(f"‚Üí Loaded {label} via joblib.load from {path}")
    except Exception as e:
        print(f"[ERROR] Loading model {label} from {path}: {e}")

# Load Keras Tokenizer (ƒë√£ ƒë∆∞·ª£c fit v√† l∆∞u l·∫°i)
# Gi·∫£ s·ª≠ tokenizer.pkl l√† Keras Tokenizer ƒë√£ l∆∞u
try:
    tokenizer_load = joblib.load(os.path.join(model_dir, "tokenizer.pkl"))
    print("‚Üí Keras Tokenizer loaded successfully.")
except Exception as e:
    print(f"[ERROR] Loading Keras Tokenizer: {e}")
    tokenizer_load = None # ƒê·∫∑t l√† None n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c

# Load Word2Vec model
w2v_model_path = os.path.join(model_dir, "word2vec_sentiment.model")
try:
    w2v_model = Word2Vec.load(w2v_model_path)
    print("‚Üí Word2Vec model loaded successfully!")
except Exception as e:
    print(f"[ERROR] Loading Word2Vec model from {w2v_model_path}: {e}")
    w2v_model = None

print("All models & tokenizer should be ready (check for errors above)!")

# ==== LOAD RESOURCES (Abbreviations and Stopwords) ====
# S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n ƒë√£ khai b√°o ·ªü ph·∫ßn "C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N"
# N·∫øu d√πng BASE_APP_DIR:
# abbs_path = DATA_DIR / "abbreviations.xlsx"
# stopwords_path = DATA_DIR / "vietnamesestopwords.txt"
# N·∫øu d√πng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa ng∆∞·ªùi d√πng:
abbs_path = os.path.join(user_data_dir, "abbreviations.xlsx")
stopwords_path = os.path.join(user_data_dir, "vietnamesestopwords.txt")


print(f"Loading abbreviations from: {abbs_path}")
try:
    abbs_df = pd.read_excel(abbs_path, engine='openpyxl') # engine='openpyxl' n·∫øu l√† file .xlsx
    abbreviation_dict = dict(zip(abbs_df['abbreviation'].astype(str), abbs_df['meaning']))
except Exception as e:
    print(f"[ERROR] Loading abbreviations: {e}")
    abbreviation_dict = {}

print(f"Loading stopwords from: {stopwords_path}")
try:
    with open(stopwords_path, "r", encoding="utf-8") as f:
        custom_stopwords = set(f.read().splitlines()) # splitlines() an to√†n h∆°n
except Exception as e:
    print(f"[ERROR] Loading stopwords: {e}")
    custom_stopwords = set()


# ==== CRAWL FROM HASAKI ====
# Get rating for comments:
def get_star(string):
    start_index = string.find(':')
    end_index = string.find('%')
    if start_index != -1 and end_index != -1 and end_index > start_index + 1:
        try:
            return int(string[start_index+1:end_index]) / 20
        except ValueError:
            return 0 # Ho·∫∑c gi√° tr·ªã m·∫∑c ƒë·ªãnh kh√°c
    return 0


def crawl_comments_from_link(input_link_button):
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
    options = webdriver.ChromeOptions()
    options.add_argument(f"user-agent={user_agent}")
    options.add_argument("--headless") # Ch·∫°y ·∫©n tr√¨nh duy·ªát
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")


    driver = None # Kh·ªüi t·∫°o driver l√† None
    try:
        driver = webdriver.Chrome(options=options)
        sleep(random.uniform(1,3)) # uniform cho s·ªë th·ª±c
        
        name_comment_all, content_comment_all, product_variant_all, datetime_comment_all, rating_comment_all = [], [], [], [], []
        
        driver.get(input_link_button)
        sleep(random.uniform(5,7))
        
        elem = driver.find_element(By.CSS_SELECTOR, 'meta[name="product:retailer_item_id"]')
        product_id = elem.get_attribute('content')
        
        elems_data_productids_list = driver.find_elements(By.CSS_SELECTOR, 'div.w-full.flex.flex-wrap.gap-2\\.5.text-xs a.relative')
        uniq_data_productids_list = [elem.get_attribute('href') for elem in elems_data_productids_list]
        product_ids_list = [re.search(r'-(\d+)\.html', url).group(1) for url in uniq_data_productids_list if re.search(r'-(\d+)\.html', url)]
        
        elems_cmtpage_nums = driver.find_elements(By.CSS_SELECTOR, '.pagination_comment a')
        if elems_cmtpage_nums:
            commentpage_nums = [int(elem.get_attribute('rel')) for elem in elems_cmtpage_nums
                                if elem.get_attribute('rel') and elem.get_attribute('rel').isdigit()]
            max_cmtpage = max(commentpage_nums) if commentpage_nums else 1
        else:
            max_cmtpage = 1

        for page_num in range(1, max_cmtpage + 1):
            try:
                sleep(random.uniform(2,3))
                print(f"Crawl Page {page_num}/{max_cmtpage}")
                
                current_names = [elem.text for elem in driver.find_elements(By.CSS_SELECTOR , ".mt-2\\.5 .pb-2\\.5 .font-bold")]
                name_comment_all.extend(current_names)

                elems_content = driver.find_elements(By.CSS_SELECTOR, ".mt-\\[5px\\]")
                new_comments_on_page = []
                for elem in elems_content:
                    try:
                        full_text = elem.text.strip()
                        # C·ªë g·∫Øng lo·∫°i b·ªè ph·∫ßn tr·∫£ l·ªùi c·ªßa qu·∫£n tr·ªã vi√™n (n·∫øu c√≥)
                        # Selector n√†y c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh t√πy theo c·∫•u tr√∫c HTML th·ª±c t·∫ø c·ªßa Hasaki
                        reply_elems = elem.find_elements(By.CSS_SELECTOR, ".mt-2\\.5.pl-\\[34px\\]") # V√≠ d·ª• selector cho ph·∫ßn reply
                        reply_text = ""
                        if reply_elems:
                            reply_text = reply_elems[0].text.strip()
                        
                        comment_only = full_text.replace(reply_text, '').strip() if reply_text else full_text
                    except NoSuchElementException: # N·∫øu kh√¥ng c√≥ ph·∫ßn reply
                        comment_only = elem.text.strip()
                    
                    if comment_only: # Ch·ªâ th√™m n·∫øu b√¨nh lu·∫≠n kh√¥ng r·ªóng
                        new_comments_on_page.append(comment_only)
                content_comment_all.extend(new_comments_on_page)
                
                current_variants = [elem.text for elem in driver.find_elements(By.CSS_SELECTOR , "div.mt-2\\.5 div.flex.items-center.gap-2 div.text-\\[\\#999\\]")]
                product_variant_all.extend(current_variants)
                
                current_datetimes = [elem.text for elem in driver.find_elements(By.CSS_SELECTOR , "div.mt-2\\.5 div.flex.items-center.gap-2 div.text-\\[\\#666\\]")]
                datetime_comment_all.extend(current_datetimes)

                current_ratings = [get_star(elem.get_attribute('style')) for elem in driver.find_elements(By.CSS_SELECTOR , "div.mt-2\\.5 div.flex.items-center.gap-2 div.relative.flex.items-center div.absolute")]
                rating_comment_all.extend(current_ratings)
                
                if page_num < max_cmtpage:
                    next_pagination_cmt = driver.find_element(By.XPATH, "//button[contains(@class, 'ml-[-3px]') and not(@disabled)]") # T√¨m n√∫t next kh√¥ng b·ªã disabled
                    actions = ActionChains(driver)
                    actions.move_to_element(next_pagination_cmt).click().perform()
                    print("Clicked on button next page!")
                    sleep(random.uniform(2,3))
                else:
                    print("Last page reached.")
                    break

            except ElementNotInteractableException:
                print("Element Not Interactable Exception on page " + str(page_num) + " ! Likely end of comments or popup.")
                break
            except NoSuchElementException:
                print("Next page button not found or other element missing on page " + str(page_num) + "!")
                break
        
        # ƒê·∫£m b·∫£o t·∫•t c·∫£ c√°c danh s√°ch c√≥ c√πng ƒë·ªô d√†i b·∫±ng c√°ch c·∫Øt b·ªõt theo danh s√°ch ng·∫Øn nh·∫•t (th∆∞·ªùng l√† content_comment)
        min_len = min(
            len(name_comment_all), 
            len(content_comment_all), 
            len(product_variant_all), 
            len(datetime_comment_all), 
            len(rating_comment_all)
        )
        comment_data = pd.DataFrame({
            'name_comment': name_comment_all[:min_len], 
            'content_comment': content_comment_all[:min_len],
            'product_variant': product_variant_all[:min_len], 
            'datetime_comment': datetime_comment_all[:min_len], 
            'rating': rating_comment_all[:min_len]
        })  
        
        comment_data.insert(0, "link_item", input_link_button)
        comment_data.insert(1, "data_product_id_list", [product_ids_list] * len(comment_data))
        comment_data.insert(2, "data_product_id", product_id)
        
        sleep(random.uniform(1,2))
        return comment_data

    except Exception as e:
        st.error(f"L·ªói trong qu√° tr√¨nh crawl: {e}")
        if driver:
            driver.quit()
        return pd.DataFrame() # Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu c√≥ l·ªói
    finally:
        if driver:
            driver.quit()


# ==== TEXT PREPROCESSING ====
def preprocess_text(text, current_abbreviation_dict, current_stopwords):
    if not isinstance(text, str):
        return [] # Tr·∫£ v·ªÅ list r·ªóng n·∫øu kh√¥ng ph·∫£i string
    text = text.lower()
    for abbreviation, meaning in current_abbreviation_dict.items():
        text = re.sub(r'\b' + re.escape(str(abbreviation)) + r'\b', str(meaning), text) # ƒê·∫£m b·∫£o abbreviation v√† meaning l√† string
    text = emoji.demojize(text, language='en') # Th√™m language='vi' n·∫øu c√≥ th·ªÉ
    text = re.sub(r"[!@#$\[\]()]", "", text) # Lo·∫°i b·ªè c√°c k√Ω t·ª± n√†y
    text = re.sub(r'[^\w\s_]', '', text) # Gi·ªØ l·∫°i d·∫•u g·∫°ch d∆∞·ªõi v√¨ ViTokenizer c√≥ th·ªÉ c·∫ßn
    tokenized_text = ViTokenizer.tokenize(text)
    tokens = [word for word in tokenized_text.split() if word not in current_stopwords]
    return tokens


# ==== VECTORIZE & PREDICT ====
# H√†m chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang vector ƒë·∫∑c tr∆∞ng b·∫±ng Word2Vec
def vectorize_comment_w2v(comment_tokens, w2v_model_instance, vector_dim=150):
    if not w2v_model_instance: # Ki·ªÉm tra n·∫øu w2v_model kh√¥ng ƒë∆∞·ª£c t·∫£i
        return np.zeros(vector_dim)
    
    words = comment_tokens # comment_tokens ƒë√£ l√† list c√°c t·ª´
    word_vectors = [w2v_model_instance.wv[word] for word in words if word in w2v_model_instance.wv.index_to_key]
    if not word_vectors: # S·ª≠a: len(word_vectors) == 0 th√†nh not word_vectors
        return np.zeros(vector_dim)
    return np.mean(word_vectors, axis=0)

# √Ånh x·∫° k·∫øt qu·∫£ d·ª± ƒëo√°n (s·ªë) sang nh√£n (text)
# B·∫†N C·∫¶N T√ôY CH·ªàNH CHO PH√ô H·ª¢P V·ªöI M√î H√åNH C·ª¶A B·∫†N
# V√≠ d·ª•: {0: "Ti√™u c·ª±c", 1: "Trung t√≠nh", 2: "T√≠ch c·ª±c"}
# Ho·∫∑c {0: "Kh√¥ng h√†i l√≤ng", 1: "H√†i l√≤ng"}
# Ho·∫∑c {0: "Kh√¥ng ƒë·ªÅ c·∫≠p", 1: "Ti√™u c·ª±c", 2: "Trung t√≠nh", 3: "T√≠ch c·ª±c"}
# Ph·∫£i ƒë·∫£m b·∫£o mapping n√†y kh·ªõp v·ªõi c√°ch model c·ªßa b·∫°n ƒë∆∞·ª£c hu·∫•n luy·ªán
reverse_label_mapping = {
    0: "Kh√¥ng ƒë·ªÅ c·∫≠p / Trung t√≠nh", # Ho·∫∑c b·∫•t k·ª≥ nh√£n n√†o t∆∞∆°ng ·ª©ng v·ªõi output 0
    1: "Ti√™u c·ª±c",
    2: "T√≠ch c·ª±c"
    # Th√™m c√°c mapping kh√°c n·∫øu c·∫ßn, v√≠ d·ª•:
    # 3: "R·∫•t t√≠ch c·ª±c"
}
# N·∫øu model c·ªßa b·∫°n ch·ªâ c√≥ 2 class (0: Ti√™u c·ª±c, 1: T√≠ch c·ª±c) th√¨ s·ª≠a l·∫°i:
# reverse_label_mapping = { 0: "Ti√™u c·ª±c", 1: "T√≠ch c·ª±c" }


def predict_sentiment_for_comment(input_text):
    if not input_text.strip(): # N·∫øu b√¨nh lu·∫≠n r·ªóng
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ m·∫∑c ƒë·ªãnh (v√≠ d·ª•: "Kh√¥ng ƒë·ªÅ c·∫≠p / Trung t√≠nh" cho t·∫•t c·∫£ c√°c kh√≠a c·∫°nh)
        default_sentiment = reverse_label_mapping.get(0, "N/A")
        return {aspect: default_sentiment for aspect in model_files_keys_uppercase}

    # Ti·ªÅn x·ª≠ l√Ω chung
    processed_tokens = preprocess_text(input_text, abbreviation_dict, custom_stopwords)

    # Vector h√≥a b·∫±ng Word2Vec cho c√°c model truy·ªÅn th·ªëng (RF, SVM,...)
    # Gi·∫£ s·ª≠ vector_dim c·ªßa Word2Vec model l√† 150
    input_vector_w2v = vectorize_comment_w2v(processed_tokens, w2v_model, vector_dim=150) 
    input_vector_w2v_reshaped = np.array([input_vector_w2v]) # Reshape cho scikit-learn models

    # Chu·∫©n b·ªã input cho model NN (LSTM, GRU,...)
    # Gh√©p l·∫°i th√†nh chu·ªói duy nh·∫•t ƒë·ªÉ ƒë∆∞a v√†o tokenizer
    processed_text_joined_for_nn = " ".join(processed_tokens)
    
    padded_sequence = None
    if tokenizer_load: # Ch·ªâ th·ª±c hi·ªán n·∫øu tokenizer ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
        sequence = tokenizer_load.texts_to_sequences([processed_text_joined_for_nn])
        max_length = 100 # C·∫ßn kh·ªõp v·ªõi max_length l√∫c hu·∫•n luy·ªán model NN
        padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')
    else: # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p tokenizer kh√¥ng t·∫£i ƒë∆∞·ª£c
        st.warning("Keras Tokenizer ch∆∞a ƒë∆∞·ª£c t·∫£i, c√°c m√¥ h√¨nh NN c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.")


    # D·ª± ƒëo√°n cho t·ª´ng kh√≠a c·∫°nh
    predictions = {}

    # QUALITY (Gi·∫£ s·ª≠ l√† model scikit-learn, d√πng Word2Vec)
    if 'quality' in models and models['quality']:
        quality_pred_raw = models['quality'].predict(input_vector_w2v_reshaped)[0]
        predictions["QUALITY"] = reverse_label_mapping.get(quality_pred_raw, f"Unknown: {quality_pred_raw}")
    else:
        predictions["QUALITY"] = "Model not loaded"

    # SERVICE (Gi·∫£ s·ª≠ l√† model Keras NN, d√πng padded_sequence)
    if 'service' in models and models['service'] and padded_sequence is not None:
        service_pred_proba = models['service'].predict(padded_sequence)
        service_pred_raw = np.argmax(service_pred_proba, axis=1)[0]
        predictions["SERVICE"] = reverse_label_mapping.get(service_pred_raw, f"Unknown: {service_pred_raw}")
    else:
        predictions["SERVICE"] = "Model/Tokenizer not loaded"
        
    # OTHERS (Gi·∫£ s·ª≠ l√† model scikit-learn, d√πng Word2Vec)
    if 'others' in models and models['others']:
        others_pred_raw = models['others'].predict(input_vector_w2v_reshaped)[0]
        predictions["OTHERS"] = reverse_label_mapping.get(others_pred_raw, f"Unknown: {others_pred_raw}")
    else:
        predictions["OTHERS"] = "Model not loaded"

    # STORE (Gi·∫£ s·ª≠ l√† model scikit-learn, d√πng Word2Vec)
    if 'store' in models and models['store']:
        store_pred_raw = models['store'].predict(input_vector_w2v_reshaped)[0]
        predictions["STORE"] = reverse_label_mapping.get(store_pred_raw, f"Unknown: {store_pred_raw}")
    else:
        predictions["STORE"] = "Model not loaded"

    # PACKAGING (Gi·∫£ s·ª≠ l√† model scikit-learn, d√πng Word2Vec)
    if 'packaging' in models and models['packaging']:
        packaging_pred_raw = models['packaging'].predict(input_vector_w2v_reshaped)[0]
        predictions["PACKAGING"] = reverse_label_mapping.get(packaging_pred_raw, f"Unknown: {packaging_pred_raw}")
    else:
        predictions["PACKAGING"] = "Model not loaded"

    # PRICE (Gi·∫£ s·ª≠ l√† model Keras NN, d√πng padded_sequence)
    if 'price' in models and models['price'] and padded_sequence is not None:
        price_pred_proba = models['price'].predict(padded_sequence)
        price_pred_raw = np.argmax(price_pred_proba, axis=1)[0]
        predictions["PRICE"] = reverse_label_mapping.get(price_pred_raw, f"Unknown: {price_pred_raw}")
    else:
        predictions["PRICE"] = "Model/Tokenizer not loaded"
        
    # In ra k·∫øt qu·∫£ (t√πy ch·ªçn, c√≥ th·ªÉ x√≥a n·∫øu kh√¥ng c·∫ßn log ·ªü console)
    # print(f"--- Predictions for: '{input_text[:50]}...' ---")
    # for aspect, sentiment in predictions.items():
    #     print(f"{aspect}: {sentiment}")

    return predictions

def predict_all_labels(comment_list):
    # Kh·ªüi t·∫°o dictionary ƒë·ªÉ l∆∞u k·∫øt qu·∫£, m·ªói kh√≠a c·∫°nh l√† m·ªôt list c√°c d·ª± ƒëo√°n
    # Chuy·ªÉn key c·ªßa model_files th√†nh ch·ªØ hoa ƒë·ªÉ nh·∫•t qu√°n v·ªõi output
    aspect_keys_uppercase = [key.upper() for key in model_files_keys_uppercase]
    all_predictions = {aspect_key: [] for aspect_key in aspect_keys_uppercase}

    if not w2v_model or not tokenizer_load:
        st.error("Word2Vec model ho·∫∑c Keras Tokenizer ch∆∞a ƒë∆∞·ª£c t·∫£i. Kh√¥ng th·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n.")
        # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh cho t·∫•t c·∫£ b√¨nh lu·∫≠n
        default_sentiment = reverse_label_mapping.get(0, "N/A") # Ho·∫∑c m·ªôt gi√° tr·ªã l·ªói
        for aspect_key in all_predictions.keys():
            all_predictions[aspect_key] = [default_sentiment] * len(comment_list)
        return all_predictions


    with st.spinner("ƒêang ph√¢n t√≠ch t·ª´ng b√¨nh lu·∫≠n..."):
        progress_bar = st.progress(0)
        total_comments = len(comment_list)
        for i, comment_text in enumerate(comment_list):
            single_comment_predictions = predict_sentiment_for_comment(comment_text)
            for aspect_key_upper in aspect_keys_uppercase: # PACKAGING, PRICE,...
                # T√¨m key g·ªëc (th∆∞·ªùng) trong single_comment_predictions
                # (v√≠ d·ª•: single_comment_predictions c√≥ key 'PRICE', 'QUALITY')
                if aspect_key_upper in single_comment_predictions:
                     all_predictions[aspect_key_upper].append(single_comment_predictions[aspect_key_upper])
                else: # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p key kh√¥ng kh·ªõp (√≠t khi x·∫£y ra n·∫øu logic ƒë√∫ng)
                    all_predictions[aspect_key_upper].append("L·ªói key")
            progress_bar.progress((i + 1) / total_comments)
    return all_predictions


# ==== STREAMLIT APP ====
st.title("üõçÔ∏è Ph√¢n t√≠ch b√¨nh lu·∫≠n s·∫£n ph·∫©m t·ª´ link Hasaki")

product_link = st.text_input("üîó Nh·∫≠p link s·∫£n ph·∫©m Hasaki:", placeholder="V√≠ d·ª•: https://hasaki.vn/san-pham/ten-san-pham-12345.html")

if 'comments_df' not in st.session_state:
    st.session_state.comments_df = pd.DataFrame()
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False


if st.button("üöÄ Ph√¢n t√≠ch b√¨nh lu·∫≠n"):
    if product_link.strip() and ("hasaki.vn/san-pham/" in product_link or "hasaki.vn/p/" in product_link) :
        with st.spinner("‚è≥ ƒêang thu th·∫≠p d·ªØ li·ªáu b√¨nh lu·∫≠n t·ª´ Hasaki... Vui l√≤ng ch·ªù trong gi√¢y l√°t."):
            comments_df_crawled = crawl_comments_from_link(product_link)
        
        if comments_df_crawled.empty:
            st.warning("‚ùó Kh√¥ng t√¨m th·∫•y b√¨nh lu·∫≠n n√†o ho·∫∑c c√≥ l·ªói trong qu√° tr√¨nh crawl t·ª´ s·∫£n ph·∫©m n√†y.")
            st.session_state.analysis_done = False
        else:
            st.success(f"üîç Thu th·∫≠p xong! T√¨m th·∫•y {len(comments_df_crawled)} b√¨nh lu·∫≠n.")
            st.session_state.comments_df = comments_df_crawled
            
            comment_list = st.session_state.comments_df['content_comment'].fillna("").astype(str).tolist()
            
            if not comment_list:
                st.warning("Danh s√°ch b√¨nh lu·∫≠n r·ªóng sau khi tr√≠ch xu·∫•t.")
                st.session_state.analysis_done = False
            else:
                pred_results_dict = predict_all_labels(comment_list) # ƒê√¢y l√† dict of lists
                
                # G√°n k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o DataFrame
                for label_aspect_upper, sentiments_list in pred_results_dict.items():
                    # ƒê·∫£m b·∫£o ƒë·ªô d√†i c·ªßa list sentiments kh·ªõp v·ªõi s·ªë d√≤ng DataFrame
                    if len(sentiments_list) == len(st.session_state.comments_df):
                        st.session_state.comments_df[label_aspect_upper] = sentiments_list
                    else:
                        st.error(f"L·ªói ƒë·ªô d√†i khi g√°n c·ªôt {label_aspect_upper}. D·ª± ki·∫øn {len(st.session_state.comments_df)}, nh·∫≠n ƒë∆∞·ª£c {len(sentiments_list)}")
                        # C√≥ th·ªÉ g√°n m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh ho·∫∑c x·ª≠ l√Ω kh√°c
                        st.session_state.comments_df[label_aspect_upper] = ["L·ªói ƒë·ªô d√†i"] * len(st.session_state.comments_df)

                st.session_state.analysis_done = True
    else:
        st.error("‚ùå Vui l√≤ng nh·∫≠p link s·∫£n ph·∫©m Hasaki h·ª£p l·ªá.")
        st.session_state.analysis_done = False

if st.session_state.analysis_done and not st.session_state.comments_df.empty:
    st.subheader("üìÑ K·∫øt qu·∫£ ph√¢n lo·∫°i b√¨nh lu·∫≠n")
    # Hi·ªÉn th·ªã c√°c c·ªôt c·∫ßn thi·∫øt, bao g·ªìm c·∫£ c√°c c·ªôt d·ª± ƒëo√°n
    display_cols = ['content_comment', 'rating'] + [col for col in st.session_state.comments_df.columns if col.isupper() and col in model_files_keys_uppercase]
    # T·∫°o model_files_keys_uppercase ƒë·ªÉ s·ª≠ d·ª•ng ·ªü ƒë√¢y
    # V√≠ d·ª•: model_files_keys_uppercase = [k.upper() for k in model_files.keys()]
    # T·∫°m th·ªùi d√πng list c√°c key ƒë√£ bi·∫øt:
    aspect_display_cols = [key.upper() for key in model_files_keys_uppercase]
    cols_to_show = ['content_comment', 'rating'] + aspect_display_cols
    st.dataframe(st.session_state.comments_df[cols_to_show])

    st.subheader("üìä Th·ªëng k√™ c·∫£m x√∫c theo t·ª´ng kh√≠a c·∫°nh")
    # C√°c c·ªôt kh√≠a c·∫°nh (ƒë√£ ƒë∆∞·ª£c th√™m v√†o df v·ªõi t√™n vi·∫øt hoa)
    label_cols_for_viz = aspect_display_cols

    # T·∫°o c√°c tab cho m·ªói kh√≠a c·∫°nh
    tabs = st.tabs(label_cols_for_viz)

    for i, aspect_col_name in enumerate(label_cols_for_viz):
        with tabs[i]:
            if aspect_col_name in st.session_state.comments_df.columns:
                st.markdown(f"#### Kh√≠a c·∫°nh: {aspect_col_name.capitalize()}")
                sentiment_counts = st.session_state.comments_df[aspect_col_name].value_counts()
                
                if not sentiment_counts.empty:
                    fig, ax = plt.subplots(figsize=(8, 5))
                    sentiment_counts.plot(kind='bar', ax=ax, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'violet'])
                    ax.set_ylabel("S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n")
                    ax.set_title(f"Ph√¢n b·ªï c·∫£m x√∫c cho kh√≠a c·∫°nh: {aspect_col_name.capitalize()}")
                    plt.xticks(rotation=45, ha="right")
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    st.write("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã cho kh√≠a c·∫°nh n√†y.")
            else:
                 st.write(f"Kh√¥ng t√¨m th·∫•y c·ªôt d·ªØ li·ªáu cho kh√≠a c·∫°nh: {aspect_col_name}")


    st.subheader("‚òÅÔ∏è Word Cloud t·ª´ t·∫•t c·∫£ b√¨nh lu·∫≠n")
    # L·∫•y l·∫°i danh s√°ch b√¨nh lu·∫≠n t·ª´ DataFrame (n·∫øu ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t)
    comment_list_for_wc = st.session_state.comments_df['content_comment'].fillna("").astype(str).tolist()
    if comment_list_for_wc:
        all_processed_tokens = []
        for comment_text in comment_list_for_wc:
            tokens = preprocess_text(comment_text, abbreviation_dict, custom_stopwords)
            all_processed_tokens.extend(tokens)
        
        if all_processed_tokens:
            cleaned_text_for_wc = ' '.join(all_processed_tokens)
            try:
                # Ki·ªÉm tra font_path n·∫øu d√πng font ti·∫øng Vi·ªát cho WordCloud
                # font_path_wc = "arial.ttf" # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n font ph√π h·ª£p n·∫øu c·∫ßn
                wc = WordCloud(width=800, height=400, background_color='white',
                               # font_path=font_path_wc, # B·ªè comment n·∫øu c√≥ font
                               collocations=False).generate(cleaned_text_for_wc)
                fig_wc, ax_wc = plt.subplots()
                ax_wc.imshow(wc, interpolation="bilinear")
                ax_wc.axis("off")
                st.pyplot(fig_wc)
            except Exception as e_wc:
                st.error(f"L·ªói t·∫°o Word Cloud: {e_wc}")
                st.write("N·ªôi dung ƒë√£ x·ª≠ l√Ω cho Word Cloud (ki·ªÉm tra): ", cleaned_text_for_wc[:500])

        else:
            st.write("Kh√¥ng c√≥ t·ª´ n√†o ƒë·ªÉ t·∫°o Word Cloud sau khi x·ª≠ l√Ω.")
    else:
        st.write("Kh√¥ng c√≥ b√¨nh lu·∫≠n n√†o ƒë·ªÉ t·∫°o Word Cloud.")

# Th√™m key model_files_keys_uppercase v√†o cu·ªëi (v√≠ d·ª•)
# ƒê√¢y l√† m·ªôt c√°ch t·∫°m ƒë·ªÉ code ch·∫°y, b·∫°n n√™n qu·∫£n l√Ω keys t·ªët h∆°n
# model_files_keys_uppercase = [k.upper() for k in model_files.keys()]